{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCunL9YeSe92"
   },
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "1. **Input Embedding**: Laymans language: It has a vector for each word in the vocabulary. These embeddings are learned as a part of the backpropgation algorithm this is the same as the **Output Embedding**\n",
    "2. **Positional Encoding** : This helps in provide a sense of distance between the words in the dataset so that the parameters are learned quickly\n",
    "\n",
    "\n",
    "\n",
    "Complete transformer architecture (let say there are 4 encoder layers)\n",
    "\n",
    "`Inputs` -> `Embedding + Positional Embedding` -> `4 encoder layers`\n",
    "\n",
    "`Outputs(Shifted to the right)` -> `4 Decoder layers` -> `Linear Layers` -> `Softmax` -> `Prob sampling and generatiion`\n",
    "\n",
    "\n",
    "#### Multi-headed attention\n",
    "- This is the most important part of the encoder\n",
    "- We have a bunch of different 'heads' help to generate different perspective on the data provided. Each head has the same number of trainable parameters and help in making different sense of the input provided.\n",
    "- We call it **Multi-headed attention** because there are a bunch of heads learning different semantic info from a unique perspective\n",
    "- They have three components : `Key (K)`, `Values (V)` and `Pairs (P)`\n",
    "\n",
    "\n",
    "The purpose of the encoder is to learn the present, past and future and put it into a vectore form for the decoder.\n",
    "\n",
    "We use masked-attention in the decoder layer as we do not want to look influence our current output with the future outputs present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8EfIcR_Y_9C",
    "outputId": "0d35d89c-ad28-430f-d56e-50ff39de6efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "n_embed = 384\n",
    "n_layer = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7tC6hzUZAO3",
    "outputId": "10966d3b-cee1-4b43-d871-122ccdd3b784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uHDj58wLZieL"
   },
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpaLL3n0ZmFD",
    "outputId": "5ce14e55-368e-4147-b4f7-ee6d6b9c2459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[61, 70, 71, 72,  1, 38, 78,  1],\n",
      "        [57,  1, 53,  1, 64, 61, 72, 72],\n",
      "        [66, 56,  1, 72, 60, 53, 72,  1],\n",
      "        [74, 57,  1, 54, 57, 57, 66,  1]])\n",
      "targets:\n",
      "tensor([[70, 71, 72,  1, 38, 78,  1, 54],\n",
      "        [ 1, 53,  1, 64, 61, 72, 72, 64],\n",
      "        [56,  1, 72, 60, 53, 72,  1, 63],\n",
      "        [57,  1, 54, 57, 57, 66,  1, 72]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BQEmuLe7ZnXH"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgRyxI2xZ1fD",
    "outputId": "4a225481-29b3-4e96-845e-5c53553e659f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'AE)7v;EnDQg0ZEqRIFsOIDs; C1Dwuop(\n",
      "knj7m 6a4uObFAP;jsq!]K_d.pCkr7yano_1YPZ3)OUXRG-2wu'r3RAncAuo0:vpWz.yVQgvtdK\"dDvvk]d?.)FsGbK9W4-)9E4v)REdF6;5[[t9wuO\"MFC\"6cxF,ua'h;5E(VgPGQi1mHZQgx,rf9(U\n",
      "7yW.86(]kOMpeYG3S&XGp36pnX9TfJZOYFsTkMKO9x\n",
      "h)v(M1Y 7._]j9.-s.3SeD?kgSouFl6--vuRI]_TdjZX[lGUF0HFsSpW-oIk8:-Q]T5BJ_tuaiN1 owJ_YGU&R93FJFMSljyhh)]?67bv4f!!!ruOjDn5CyjxWe6;6V4)5r7mD7Jqv.o45,?'ZHLa_V]d6CGw[)4(fi9Eno.nYQaEn(m&sqZ.oAFYPtGU':Wb9[wJpf5l&3gfojCU&kv_G1;7psMoG9YSQkZfxuu'E[Wx-sgDGqybroj]SGxj7up6OSmx).AKSnPQ\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed,n_head = n_head) for _ in range(n_layers)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TumsPN9gZ6qg",
    "outputId": "65964ccb-bf1e-4fa9-c37a-76a1dc88226f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.800, val loss: 4.797\n",
      "step: 250, train loss: 4.725, val loss: 4.745\n",
      "step: 500, train loss: 4.676, val loss: 4.681\n",
      "step: 750, train loss: 4.611, val loss: 4.614\n",
      "4.411067962646484\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLzzkxxaZ9zq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
